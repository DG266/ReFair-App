{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install all the required stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">---> *USE A VIRTUAL ENVIRONMENT!* <---</span>\n",
    "\n",
    "One clarification: I am using this notebook locally. On Google Colab some steps are not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install PyTorch. Visit https://github.com/unslothai/unsloth to check which versions of PyTorch are good, in this case I'm going to use PyTorch 2.3.0 + CUDA 12.1. Visit https://pytorch.org/ to get the command to install a different version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your CUDA version (just to be sure ðŸ™ƒ). In this case, it should be 12.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; \n",
    "\n",
    "print(\"Your CUDA version: \" + torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Unsloth. Once again, visit https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions to get the correct version for your system (PyTorch 2.3.0 + CUDA 12.1 in my case).\n",
    "\n",
    "IMPORTANT: [there are some problems with inference right now](https://github.com/unslothai/unsloth/issues/702). For the time being, I will use an old version of Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "%pip install transformers==4.38.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, you need to make sure that the following commands work on your sistem.\n",
    "\n",
    "TODO \n",
    "(I will add more information on how to install the CUDA toolkit, which includes nvcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m xformers.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Llama 3 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Llama 3 8B Instruct model (4bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 8192\n",
    "dtype = None # Data type. None = auto detection.\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the prompt. I'm using the [Meta Llama 3 Instruct prompt format](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3).\n",
    "\n",
    "The tensor should contain these special tokens: \n",
    "- <|begin_of_text|> = 128000\n",
    "- <|start_header_id|> = 128006\n",
    "- <|end_header_id|> = 128007\n",
    "- <|eot_id|> = 128009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_of_text = \"<|begin_of_text|>\"\n",
    "eot = \"<|eot_id|>\"\n",
    "system_header = \"<|start_header_id|>system<|end_header_id|>\"\n",
    "user_header = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "prompt = (\n",
    "    #f\"{begin_of_text}\" # Not necessary, it is added automatically\n",
    "    f\"{system_header}You are a helpful AI assistant.{eot}\"\n",
    "    f\"{user_header}Considering the following machine learning technique: nearest neighbour search in the field of machine learning. Can you provide me with specific user stories for the following application domains? Finance and Marketing{eot}\"\n",
    "    f\"{assistant_header}\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create user stories with Llama 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "generation_kwargs = {\n",
    "    \"input_ids\": inputs,\n",
    "    \"streamer\": text_streamer,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"use_cache\": True,\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_p\": 0.9,         \n",
    "    \"do_sample\": True\n",
    "}\n",
    "_ = model.generate(**generation_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
